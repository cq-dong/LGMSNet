{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1 - x1_raw and x1_step1 are equal: True\n",
      "Step 2 - x1_step1 and x1_step2 are equal: True\n",
      "x1 input and output are equal: True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x1_raw = torch.randn(1, 64, 256, 256)\n",
    "b, c, h1, w1 = x1_raw.size()\n",
    "\n",
    "# Step 1: Reshape to [b, c, h1/8*w1/8, 8*8]\n",
    "x1_step1 = x1_raw.view(b, c, -1, 8, 8).contiguous().view(b, c, -1, 8*8).contiguous()\n",
    "print(\"Step 1 - x1_raw and x1_step1 are equal:\", torch.allclose(x1_raw, x1_step1.view(b, c, h1, w1), atol=1e-6))\n",
    "\n",
    "# Step 2: Reshape to [b, c, h1/8, w1/8, 8, 8]\n",
    "x1_step2 = x1_step1.view(b, c, h1 // 8, w1 // 8, 8, 8).contiguous()\n",
    "print(\"Step 2 - x1_step1 and x1_step2 are equal:\", torch.allclose(x1_raw, x1_step2.view(b, c, -1, 8*8).view(b, c, h1, w1), atol=1e-6))\n",
    "\n",
    "# Step 4: Reshape to [b, c, h1, w1]\n",
    "x1_step3 = x1_step2.view(b, c, h1, w1).contiguous()\n",
    "# print(\"Step 4 - x1_step3 and x1_step4 are equal:\", torch.allclose(x1_step3.view(b, c, h1 // 8, w1 // 8, 8, 8), x1_step4.view(b, c, h1 // 8, w1 // 8, 8, 8), atol=1e-6))\n",
    "\n",
    "# Final check\n",
    "print(\"x1 input and output are equal:\", torch.allclose(x1_raw, x1_step3, atol=1e-6))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([65536, 85]) 153\n",
      "tensor([[[[ 0.0000,  0.0000,  0.0000,  ...,  0.5856, -1.2384,  0.6905],\n",
      "          [-1.1543,  0.0142,  0.9163,  ...,  1.8587,  1.0796, -0.8682],\n",
      "          [ 0.2547, -1.3659, -0.2858,  ..., -0.9787,  0.5877,  0.1279],\n",
      "          ...,\n",
      "          [-0.6535, -0.2089,  2.4657,  ...,  1.0804, -1.5527,  0.9178],\n",
      "          [ 1.6886,  1.2923, -0.1877,  ..., -1.6766,  1.6754,  0.4312],\n",
      "          [ 1.4011,  0.8134,  0.3454,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000,  ...,  1.3305, -0.4923,  1.6294],\n",
      "          [ 0.6098, -1.1154,  0.4179,  ...,  0.4997, -0.3057, -0.3300],\n",
      "          [-0.9073,  0.8674, -0.8304,  ...,  0.0151, -1.0819, -0.5116],\n",
      "          ...,\n",
      "          [-0.1777, -0.7293,  0.9552,  ...,  2.2984,  1.5865,  2.3561],\n",
      "          [ 1.7521, -1.7325, -0.0658,  ...,  0.5796, -1.5226,  0.7169],\n",
      "          [-1.4531,  0.7956, -0.0077,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000,  ..., -0.7846,  0.6745, -0.7153],\n",
      "          [-1.2503,  0.9151,  0.9037,  ..., -1.2433,  1.4300,  0.1311],\n",
      "          [ 0.0651,  0.4747,  0.0418,  ..., -0.9147,  0.6149,  1.4821],\n",
      "          ...,\n",
      "          [ 0.9123, -2.1135,  1.6480,  ..., -2.2046, -0.0810,  1.0913],\n",
      "          [ 1.2836, -0.6078,  0.0549,  ...,  1.0452,  1.8247, -0.9159],\n",
      "          [-2.0949, -0.7482,  0.2213,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000,  ..., -2.2266,  1.2536,  2.2100],\n",
      "          [-0.2263, -0.3293,  1.4684,  ...,  1.1845, -0.4490, -1.0634],\n",
      "          [-2.3280, -0.9387,  1.8231,  ..., -0.2129,  0.1273, -1.9242],\n",
      "          ...,\n",
      "          [-0.5278, -1.2462, -0.2238,  ..., -1.5224, -0.9060,  0.3257],\n",
      "          [ 0.8218, -0.2862,  2.4208,  ...,  0.7051,  3.2130, -0.5471],\n",
      "          [ 0.4926,  0.2296, -0.6407,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000,  ...,  1.9848,  1.9547, -1.1522],\n",
      "          [ 1.8804,  2.2922, -2.0328,  ..., -1.1262,  0.0362, -1.5686],\n",
      "          [ 1.6496,  1.3175, -0.2293,  ..., -1.6499, -1.6547, -0.2485],\n",
      "          ...,\n",
      "          [ 1.6513, -2.9975, -0.3771,  ..., -1.2816, -1.0626,  1.0167],\n",
      "          [-0.9709, -0.7297, -0.1846,  ...,  1.6552, -0.2321,  3.4287],\n",
      "          [ 0.8549, -1.1436, -1.4961,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000,  ..., -3.1271, -1.5426, -1.8867],\n",
      "          [ 1.6078,  0.8981,  0.4990,  ...,  2.3390,  0.4976, -0.3792],\n",
      "          [ 1.0322,  0.5702, -0.9131,  ...,  0.1018, -1.4050,  0.5408],\n",
      "          ...,\n",
      "          [ 0.9122,  0.0065, -1.3070,  ...,  0.7385,  0.2523,  1.5012],\n",
      "          [ 0.1055, -0.9884,  1.6001,  ...,  0.8722,  1.0734,  1.2575],\n",
      "          [ 1.4934, -0.4737,  1.4690,  ...,  0.0000,  0.0000,  0.0000]]]])\n"
     ]
    }
   ],
   "source": [
    "x2 = x2_raw.view(b, c, -1, 4, 4).view(b, c, -1, 4*4)  # [b, C, h1/8*w1/8, 4*4]\n",
    "x3 = x3_raw.view(b, c, -1, 2, 2).view(b, c, -1, 2*2)  # [b, C, h1/8*w1/8, 2*2]\n",
    "x4 = x4_raw.view(b, c, -1, 1, 1).view(b, c, -1, 1*1)  # [b, C, h1/8*w1/8, 1*1]\n",
    "\n",
    "# Concatenate along the last dimension\n",
    "x = torch.cat((x1, x2, x3, x4), dim=-1)  # [b, C, h1/8*w1/8, 8*8 + 4*4 + 2*2 + 1*1]\n",
    "\n",
    "# Flatten for MLP\n",
    "x = x.view(-1, 8*8 + 4*4 + 2*2 + 1*1)  # [b*C,  (8*8 + 4*4 + 2*2 + 1*1)]\n",
    "\n",
    "# Apply MLP\n",
    "print(x.shape,\"153\")\n",
    "# x = self.mlp(x)  # [b, C * (8*8 + 4*4 + 2*2 + 1*1)]\n",
    "\n",
    "# Reshape back to original dimensions\n",
    "x = x.view(b, c, -1, 8*8 + 4*4 + 2*2 + 1*1)  # [b, C, h1/8*w1/8, 8*8 + 4*4 + 2*2 + 1*1]\n",
    "\n",
    "# Split back into individual tensors\n",
    "x1, x2, x3, x4 = x.split([8*8, 4*4, 2*2, 1*1], dim=-1)\n",
    "\n",
    "# Reshape back to original spatial dimensions\n",
    "x1 = x1.view(b, c, h1 // 8, w1 // 8, 8, 8).permute(0,1,2,4,3,5).contiguous().view(b, c, h1, w1)  # [b, C, h1, w1] \n",
    "x2 = x2.view(b, c, h1 // 8, w1 // 8, 4, 4).permute(0,1,2,4,3,5).contiguous().view(b, c, h1 // 2, w1 // 2)  # [b, C, h1/2, w1/2]\n",
    "x3 = x3.view(b, c, h1 // 8, w1 // 8, 2, 2).permute(0,1,2,4,3,5).contiguous().view(b, c, h1 // 4, w1 // 4)  # [b, C, h1/2, w1/4]\n",
    "x4 = x4.view(b, c, h1 // 8, w1 // 8, 1, 1).permute(0,1,2,4,3,5).contiguous().view(b, c, h1 // 8, w1 // 8)  # [b, C, h1/8, w1/8]\n",
    "print(x1_raw-x1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 256, 16, 16])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 创建维度为[256, 1]的张量\n",
    "tensor1 = torch.randn(256, 1,1)\n",
    "\n",
    "# 创建维度为[8, 256, 16, 16]的张量\n",
    "tensor2 = torch.randn(8, 256, 16, 16)\n",
    "\n",
    "# 对tensor2进行转置操作，使其满足矩阵乘法的维度匹配规则\n",
    "# tensor2_transposed = tensor2.permute(0, 2, 3, 1).view(8, -1, 256)\n",
    "\n",
    "# 进行矩阵乘法\n",
    "result = tensor1*tensor2\n",
    "\n",
    "\n",
    "\n",
    "print(result.shape)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class CMUNeXtBlock_MK(nn.Module):  # 根据SCSA灵感，直接在一个block中为不同的channel设置不同的卷积核，同时引入通道注意力\n",
    "    def __init__(self, ch_in, ch_out, depth=1, k=3): # ch_in 需要是4的倍数\n",
    "        super(CMUNeXtBlock, self).__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            *[nn.Sequential(\n",
    "                Residual(nn.Sequential(\n",
    "                    # deep wise\n",
    "                    nn.Conv2d(ch_in, ch_in, kernel_size=(k, k), groups=ch_in, padding=(k // 2, k // 2)),\n",
    "                    nn.GELU(),\n",
    "                    nn.BatchNorm2d(ch_in)\n",
    "                )),\n",
    "                nn.Conv2d(ch_in, ch_in * 4, kernel_size=(1, 1)),\n",
    "                nn.GELU(),\n",
    "                nn.BatchNorm2d(ch_in * 4),\n",
    "                nn.Conv2d(ch_in * 4, ch_in, kernel_size=(1, 1)),\n",
    "                nn.GELU(),\n",
    "                nn.BatchNorm2d(ch_in)\n",
    "            ) for i in range(depth)]\n",
    "        )\n",
    "        self.up = conv_block(ch_in, ch_out)\n",
    "        self.group_chans = group_chans = self.dim // 4\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, h_, w_ = x.size()\n",
    "        l_x_h, g_x_h_s, g_x_h_m, g_x_h_l = torch.split(x, self.group_chans, dim=1)\n",
    "        x = self.block(x)\n",
    "        x = self.up(x)\n",
    "        return x\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uxnet3d",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
